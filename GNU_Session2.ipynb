{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junkyuhufs/workshop/blob/main/GNU_Session2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🌿 특강 2: 자연어처리와 디지털인문학 따라하기\n",
        "##1. 자연언어처리(NLP) \n",
        "##2. 국정연설 토픽모델링(Topic-Modeling) \n",
        "##3. 문학작품 감성분석(Sentiment Analysis)\n",
        "##4. 영화리뷰 군집분석(Clustering Analysis)\n",
        "\n",
        "### 발표: 이준규 (한국외국어대학교 교육대학원 영어교육전공)"
      ],
      "metadata": {
        "id": "jc0IN0eAkiUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 개요"
      ],
      "metadata": {
        "id": "BDwtugcFWClP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B-eitBRrkhgJ"
      },
      "outputs": [],
      "source": [
        "#@markdown Introduction \n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [\"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.01.png\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.02.png\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.03.png\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.04.png\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.05.png\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.06.png\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 7)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"800\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Python 시작하기\n",
        "##텍스트 전처리 예시 (Preprocessing examples)"
      ],
      "metadata": {
        "id": "TxTvBlncmnkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Python library & 전처리\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [ \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.07.png\",\n",
        "         \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.08.png\",\n",
        "         \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.11.png\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 4)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"800\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JLYBUfV1rCQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Import/Install relevant packages\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Zk3mv9Tdmult"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown \"The rain in Spain falls mainly on the plain.\" 전처리\n",
        "text = \"The rain in Spain falls mainly on the plain.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.is_stop)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
        "rows = []\n",
        "\n",
        "for t in doc:\n",
        "    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
        "    rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "    \n",
        "df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Du694UkntK21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Sentence 시각화\n",
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "vckRAgRZteiP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [1] Topic modeling w/ 미국 대통령 연설문"
      ],
      "metadata": {
        "id": "id1w71eot1S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [ \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.09.png\",\n",
        "         \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.10.png\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 3)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"800\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8I-mqeH_mWWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 필요한 파일 불러오기 \n",
        "\n",
        "* 데이터: **state-of-the-union.csv:** 1970에서 2012까지 미국 대통령 국정연설"
      ],
      "metadata": {
        "id": "I_eTrbnruQJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[💾 Topic modeling 실습파일 다운받기](https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/text-analysis/data/state-of-the-union.csv)"
      ],
      "metadata": {
        "id": "QJOIZHBpuRnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 국정연설 파일 불러오기\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Make data directory if it doesn't exist\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Download the CSV file\n",
        "url = \"https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/text-analysis/data/state-of-the-union.csv\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save the content to a file\n",
        "with open(\"data/state-of-the-union.csv\", \"wb\") as f:\n",
        "    f.write(response.content)\n"
      ],
      "metadata": {
        "id": "66y8Ig6XBu8H",
        "cellView": "form"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 불러온 csv를 data frame 형태로 변환 및 청소\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"data/state-of-the-union.csv\")\n",
        "# Clean it up a little bit, removing non-word characters (numbers and ___ etc)\n",
        "df.content = df.content.str.replace(\"[^A-Za-z ]\", \" \")\n",
        "df.head()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DfQCZFFuu3fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 워드클라우드 시각화\n",
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(df.content.values))\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=36, contour_color='steelblue', width = 800, height=600)\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "In-xOrwHvSzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA (잠재 디리클레 할당)을 위한 자료 준비\n",
        "\n",
        "+ gensim.utils.simple_preprocess를 이용하여, document를 a list of tokens으로 변환. \n",
        "+ 소문자로 바꾸기, 토큰화 등 (선택사항) "
      ],
      "metadata": {
        "id": "2JJhthA8NNq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Gensim설치\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!pip install gensim"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5lFhdJTd3Mf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown gensim의 simple_preprocess 이용한 토큰화\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "df.content = df.content.apply(simple_preprocess)"
      ],
      "metadata": {
        "id": "uYBrLYqRNXuX",
        "cellView": "form"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "불용어(stopwords) 제거하기: 예) to, I, the, a, from, 등등"
      ],
      "metadata": {
        "id": "brll8tdLZzLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown stopwords 제거\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "# stop_words.extend(['from', 'to']) # add more if want\n",
        "df.content = df.content.apply(lambda words: [word for word in words if word not in stop_words])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KKVX4GPQN66Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 토큰화 > 카운트벡터 (BOW형태로 변환) 형태인 corpus 생성; 처음 20개 결과 확인\n",
        "texts = df.content #Gensim에서는 토큰화된 결과를 texts로 지정해야 함\n",
        "from gensim import corpora\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5) #출현한 문서 빈도수가 낮거나 (문서에서 5번 이하) or 높은 단어들 (0.5 -> 50%이상) 제외 \n",
        "corpus = [dictionary.doc2bow(text) for text in texts] #doc2bow() >> 토큰화된 결과를 카운트 벡터, 즉 BOW형태로 변환; Gensim에서는 doc2bow()의 결과를 corpus로 지정해야 함\n",
        "corpus[0][:20]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "h_K6oUFUOSTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 카운트 벡터형태의 corpus를 TF-IDF로 변환; 마지막 20개 결과 확인\n",
        "from gensim import models\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "corpus_tfidf[0][-20:]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oP-YqoxoO5cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown LDA(잠재 디리클레할당) 실행; 토픽수 = 15개 지정 (3개만 확인)\n",
        "from gensim import models\n",
        "n_topics = 15\n",
        "lda_model = models.ldamodel.LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=n_topics)\n",
        "\n",
        "#15개 중 3개만 보여주기\n",
        "lda_model.print_topics()[:3]"
      ],
      "metadata": {
        "id": "2LMATQYyPjTD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 시각화 라이브러리 설치\n",
        "%%capture\n",
        "!pip install pyLDAvis\n",
        "!pip install \"pandas<2.0.0\" "
      ],
      "metadata": {
        "cellView": "form",
        "id": "cx6QDKaybUQp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA 시각화"
      ],
      "metadata": {
        "id": "Ic7ap7JPb7Cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown LDA결과 시각화\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
        "vis"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j1XyigaVQEZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [2] Sentiment Analysis (w/ Harry Potter)"
      ],
      "metadata": {
        "id": "usEAgkWkRPBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Intro to SA\n",
        "\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [\"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.13.png\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.12.png\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.14.png\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 4)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"800\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RNuA9SEtRTft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 분석할 자료 준비"
      ],
      "metadata": {
        "id": "OxA50awOT-Nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ [소스링크: Harry Potter 자료를 공유한 깃허브 사이트](https://github.com/ErikaJacobs/Harry-Potter-Text-Mining.git)\n",
        "\n",
        "+ [텍스트 예시](https://raw.githubusercontent.com/ErikaJacobs/Harry-Potter-Text-Mining/master/Book%20Text/HPBook1.txt)"
      ],
      "metadata": {
        "id": "6zTPhIAnRgx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Harry Potter자료 가져오기\n",
        "!git clone https://github.com/ErikaJacobs/Harry-Potter-Text-Mining.git"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jzAN3jZORiE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 데이터 전처리: Pandas이용 데이터 정리 (책의 한 챕터가 한 셀에 있는 상태)\n",
        "import pandas as pd #Importing Pandas package\n",
        "%cd /content/Harry-Potter-Text-Mining/Book Text\n",
        "\n",
        "import glob \n",
        "fns = glob.glob('*.txt')\n",
        "df = pd.DataFrame()\n",
        "for fn in fns:\n",
        "  dftmp = pd.read_csv(fn, sep=\"@\")\n",
        "  df = pd.concat([df, dftmp])\n",
        "\n",
        "%cd /content\n",
        "\n",
        "df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Wd11Hx9_R0EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 데이터 전처리 불용어(stopwords) 제거\n",
        "import nltk #Import NLTK library\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt') #installed punkt to fix error\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords #Import stopwords to Python\n",
        "\n",
        "stopwords = set(stopwords.words('english')) #English stopwords assigned to \"stopwords\" object\n",
        "\n",
        "import string #Punctuation\n",
        "\n",
        "# Function for removing punctuation\n",
        "def remove_punctuations(text):\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "stopwords = [''.join(item for item in x if item not in string.punctuation) for x in stopwords] #Remove punctuation from stopwords\n",
        "\n",
        "df['WordCountText']=df['Text'].str.lower().apply(remove_punctuations).apply(word_tokenize) # Word Count Text\n",
        "# Word Count\n",
        "df['WordCloudText']=df['WordCountText'].apply(lambda x: [word for word in x if word not in stopwords]) # Word Cloud Text\n",
        "df['WordCount'] = df['WordCountText'].str.len() #Word Count Per Chapter"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gOMavAe8SOyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 데이터 전처리: 책 > 문장단위로 (챕터가 문장단위로 나뉜 상태)\n",
        "# Creating a table breaking down the text by each sentence, rather than each chapter.\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Make smaller table - reset index to prepare for further work\n",
        "dfsentiment = df[['Book','Chapter','Text']].reset_index() \\\n",
        "    .drop([\"index\"], axis=1)\n",
        "dfsentiment = dfsentiment.join(dfsentiment.Text.apply(sent_tokenize).rename('Sentences')) # Breaking apart text into sentences\n",
        "\n",
        "#Put every tokenized sentence into its own row\n",
        "dfsentiment2 = dfsentiment.Sentences.apply(pd.Series) \\\n",
        "    .merge(dfsentiment, left_index = True, right_index = True) \\\n",
        "    .drop([\"Text\"], axis = 1) \\\n",
        "    .drop([\"Sentences\"], axis = 1) \\\n",
        "    .melt(id_vars = ['Book', 'Chapter'], value_name = \"Sentence\") \\\n",
        "    .drop(\"variable\", axis = 1) \\\n",
        "    .dropna()\n",
        "\n",
        "# Sort new table by Book and Chapter - reset index to reflect new order\n",
        "dfsentiment2=dfsentiment2.sort_values(by=['Book', 'Chapter']) \\\n",
        "    .reset_index() \\\n",
        "    .drop(['index'], axis = 1)\n",
        "\n",
        "# Clean punctuation, lower case\n",
        "dfsentiment2['Sentence']=dfsentiment2.Sentence.apply(remove_punctuations).apply(lambda x: x.lower()) \\\n",
        "\n",
        "# Check first five values\n",
        "dfsentiment2"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zcC4zJ9JSkso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##감정분석 실행"
      ],
      "metadata": {
        "id": "T-ATz9QaUDfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 필요한 라이브러리(VADER library) 불러오기\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "sid=nltk.sentiment.vader.SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hF38sFIGUHdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Harry Potter 예시문장 (stemmed 문장)\n",
        "\n",
        "\n",
        "|출처 | 예시문장 | 감정 |\n",
        "|--|--|--|\n",
        "|[0,1,1]|'the boy who lived mr and mrs dursley of number four privet drive were **proud** to say that they were perfectly normal **thank** you very much'|😄 Positive|[0,1,1]|\n",
        "|[1,1,1]|'they were the last people youd expect to be involved in anything **strange** or **mysterious** because they just didnt hold with such **nonsense**'|😡 Negative |\n",
        "|[2,1,1]|'mr dursley was the director of a firm called grunnings which made drills'|😐 Neutral|\n",
        "\n",
        "\n",
        "+ *Note*. 출처 == [sentence number, Book, Chapter]"
      ],
      "metadata": {
        "id": "Do0r7a0tFiLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 문장별로 감정분석 점수 부여; Compound, positive, negative, neutral \n",
        "# Get intensity scores of each sentence\n",
        "dfsentiment2['Score']=dfsentiment2.Sentence.apply(lambda x: sid.polarity_scores(x))\n",
        "\n",
        "# Place scores in own columns\n",
        "dfsentiment2['CompScore']=dfsentiment2.Score.apply(lambda x: x.get(\"compound\"))\n",
        "dfsentiment2['PosScore']=dfsentiment2.Score.apply(lambda x: x.get(\"pos\"))\n",
        "dfsentiment2['NegScore']=dfsentiment2.Score.apply(lambda x: x.get(\"neg\"))\n",
        "dfsentiment2['NeuScore']=dfsentiment2.Score.apply(lambda x: x.get(\"neu\"))\n",
        "\n",
        "# With scores extracted, the original score field can be removed\n",
        "dfsentiment2 = dfsentiment2.drop([\"Score\"], axis=1)\n",
        "\n",
        "# Adding Sentiment Flags\n",
        "dfsentiment2['PosFlag'] = dfsentiment2.CompScore.apply(lambda x: 1 if x >= 0.05 else 0)\n",
        "dfsentiment2['NegFlag'] = dfsentiment2.CompScore.apply(lambda x: 1 if x <= -0.05 else 0)\n",
        "dfsentiment2['NeuFlag'] = dfsentiment2.CompScore.apply(lambda x: 1 if x < 0.05 and x > -0.05 else 0)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9wIjPpC3Uj6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfsentiment2.head(20)"
      ],
      "metadata": {
        "id": "QHL5QRuwU_f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown [1] 감정분석 결과 막대그래프 (부정, 중립, 긍정)\n",
        "\n",
        "print('* Negative Flag: ', dfsentiment2['NegFlag'].sum())\n",
        "print('* Neutral Flag: ', dfsentiment2['NeuFlag'].sum())\n",
        "print('* Positive Flag: ', dfsentiment2['PosFlag'].sum())\n",
        "print(\"=\"*50)\n",
        "print('Total: ',dfsentiment2['PosFlag'].sum()+dfsentiment2['NeuFlag'].sum()+dfsentiment2['NegFlag'].sum())\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Negative = int(dfsentiment2['NegFlag'].sum())\n",
        "Neutral = int(dfsentiment2['NeuFlag'].sum())\n",
        "Positive = int(dfsentiment2['PosFlag'].sum())\n",
        "\n",
        "# Your three integer frequencies\n",
        "freqs = [Negative, Neutral, Positive]\n",
        "# freqs = [18385, 33544, 19055]\n",
        "\n",
        "# Create labels for the bars\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "# Create x coordinates for the bars\n",
        "x = np.arange(len(labels))\n",
        "\n",
        "# Generate the bar plot\n",
        "plt.bar(x, freqs)\n",
        "\n",
        "\n",
        "# Specify the colors for each category\n",
        "colors = ['lightblue', 'gray', 'orange']\n",
        "\n",
        "# Generate the bar plot with custom colors\n",
        "\n",
        "bars = plt.bar(x, freqs, color=colors)\n",
        "# Add labels to the x-axis\n",
        "plt.xticks(x, labels)\n",
        "\n",
        "# Set axis labels\n",
        "plt.xlabel('Categories')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Set a title for the plot\n",
        "plt.title('Bar plot of Sentiment categories')\n",
        "plt.ylim(0, 40000) \n",
        "# Add the frequency text within each bar\n",
        "for bar, freq in zip(bars, freqs):\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() - 2, str(freq),\n",
        "             ha='center', va='bottom', fontsize=12, color='gray')\n",
        "\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TII9dv7EkLwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown [2] 감정분석 카테고리 파이차트 (비율확인용)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Negative = int(dfsentiment2['NegFlag'].sum())\n",
        "Neutral = int(dfsentiment2['NeuFlag'].sum())\n",
        "Positive = int(dfsentiment2['PosFlag'].sum())\n",
        "\n",
        "# Your three integer frequencies\n",
        "freqs = [Negative, Neutral, Positive]\n",
        "\n",
        "# Create labels for the segments\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "# Specify the colors for each segment\n",
        "colors = ['lightblue', 'gray', 'orange']\n",
        "\n",
        "# Generate the pie chart with custom colors\n",
        "plt.pie(freqs, labels=labels, colors=colors, autopct='%.1f%%', startangle=90)\n",
        "\n",
        "# Set a title for the plot\n",
        "plt.title('Pie chart of Sentiment categories')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "N7iyPZVEnAFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###시계열 분석 시각화"
      ],
      "metadata": {
        "id": "nz5KP9vxV5m0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 7권 개별적으로 감정변화 흐름 확인\n",
        "#Time series of sentiments in 7 books of Harry Potter\n",
        "\n",
        "dfsentiment2.groupby('Book').mean()['CompScore']\n",
        "\n",
        "def Titles(x):\n",
        "    if x == 1:\n",
        "        return \"1 - Sorcerer's Stone\"\n",
        "    if x == 2:\n",
        "        return \"2 - Chamber of Secrets\"\n",
        "    if x == 3:\n",
        "        return \"3 - Prizoner of Azkaban\"\n",
        "    if x == 4:\n",
        "        return \"4 - Goblet of Fire\"\n",
        "    if x == 5:\n",
        "        return \"5 - Order of the Phoenix\"\n",
        "    if x == 6:\n",
        "        return \"6 - Half Blood Prince\"\n",
        "    if x == 7:\n",
        "        return \"7 - Deathly Hallows\"\n",
        "\n",
        "dfsentiment2['BookTitle']=dfsentiment2.Book.apply(lambda x: Titles(x))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "colorsList = ['#DC8458', '#950702', '#8E067D', '#2E8C44', '#395196', '#60A619','#ECA10A'] #Mauraders Map Colors\n",
        "ColorMap = matplotlib.colors.ListedColormap(colorsList)\n",
        "\n",
        "# plot data\n",
        "fig, ax = plt.subplots(figsize=(10,16))\n",
        "# use unstack()\n",
        "dfsentiment2.groupby(['Chapter','BookTitle']).mean()['CompScore'].unstack().plot(ax=ax, subplots=True, ylim=(-0.25, 0.25), colormap=ColorMap)\n",
        "plt.style.use('ggplot')\n",
        "ax.set_ylabel('Compound Sentiment Score')\n",
        "\n",
        "[ax.legend(loc=1) for ax in plt.gcf().axes]"
      ],
      "metadata": {
        "id": "pcdnCk0cWACI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 7권을 통합하여 감정변화 흐름 확인\n",
        "#Time series of sentiments in 7 books of Harry Potter\n",
        "\n",
        "dfsentiment2.groupby('Book').mean()['CompScore']\n",
        "\n",
        "def Titles(x):\n",
        "    if x == 1:\n",
        "        return \"1 - Sorcerer's Stone\"\n",
        "    if x == 2:\n",
        "        return \"2 - Chamber of Secrets\"\n",
        "    if x == 3:\n",
        "        return \"3 - Prizoner of Azkaban\"\n",
        "    if x == 4:\n",
        "        return \"4 - Goblet of Fire\"\n",
        "    if x == 5:\n",
        "        return \"5 - Order of the Phoenix\"\n",
        "    if x == 6:\n",
        "        return \"6 - Half Blood Prince\"\n",
        "    if x == 7:\n",
        "        return \"7 - Deathly Hallows\"\n",
        "\n",
        "dfsentiment2['BookTitle']=dfsentiment2.Book.apply(lambda x: Titles(x))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "colorsList = ['#DC8458', '#950702', '#8E067D', '#2E8C44', '#395196', '#60A619','#ECA10A'] #Mauraders Map Colors\n",
        "ColorMap = matplotlib.colors.ListedColormap(colorsList)\n",
        "\n",
        "# plot data\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "# use unstack()\n",
        "dfsentiment2.groupby(['Chapter','BookTitle']).mean()['CompScore'].unstack().plot(ax=ax, subplots=False, ylim=(-0.3, 0.3), colormap=ColorMap)\n",
        "plt.style.use('ggplot')\n",
        "ax.set_ylabel('Compound Sentiment Score')\n",
        "\n",
        "[ax.legend(loc=2) for ax in plt.gcf().axes]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "urdwcB2diXL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ChatGPT says:** In Harry Potter and the Deathly Hallows, the main tragic event between chapters 25 and 28 is the aftermath of Dobby's death. Dobby, the house-elf, dies a hero while saving Harry and his friends from the Death Eaters at Malfoy Manor.\n",
        "\n",
        "\n",
        "+ The main tragic event that occurs between chapters 25 and 28 in Harry Potter and the Deathly Hallows (Book 7) is the death of Dobby, the house-elf. This event takes place in Chapter 23, \"Malfoy Manor\", and is not directly within the range you mentioned (chapters 25-28). However, its consequences and emotional impact on the characters continue to resonate in the following chapters.\n",
        "\n",
        "+ Dobby dies while helping Harry, Hermione, Ron, Luna, Dean, and Griphook escape from Malfoy Manor, where they were captured and held prisoner by Bellatrix Lestrange and other Death Eaters. In an attempt to rescue them, Dobby Apparates into the Manor, and in the ensuing chaos, he is fatally struck by a knife thrown by Bellatrix Lestrange as they Disapparate away.\n",
        "\n",
        "+ Dobby's death is a significant and heartbreaking moment in the story, as he dies a hero, saving Harry and his friends from the Death Eaters. It underscores the themes of loyalty, sacrifice, and the importance of valuing all beings, regardless of their background or status."
      ],
      "metadata": {
        "id": "y7Kw52z_h7DX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ChatGPT says:**\n",
        "\n",
        "**Ending story:** The ending of Harry Potter and the Deathly Hallows (Book 7) sees the final defeat of Lord Voldemort by Harry Potter during the Battle of Hogwarts. The ending is generally considered happy, as it brings the closure of the long-standing conflict between Harry and Voldemort, and most of the central characters, including Harry, Ron, and Hermione, survive and go on to live fulfilling lives. The book concludes with an epilogue set 19 years later, where Harry, Ron, Hermione, and their families are shown sending their own children off to Hogwarts School of Witchcraft and Wizardry, signifying a hopeful and peaceful future."
      ],
      "metadata": {
        "id": "21XMBehblCJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[3] Cluster analysis w/ movie reviews"
      ],
      "metadata": {
        "id": "5PeqhNiQX2F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown CA intro\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [\"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.20.png\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.21.png\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.22.png\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 4)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"800\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4GDHpqLlX6Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Import/Install relevant packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import os\n",
        "import codecs\n",
        "from sklearn import feature_extraction\n",
        "!pip install mpld3\n",
        "import mpld3\n",
        "import requests"
      ],
      "metadata": {
        "cellView": "form",
        "id": "krYuonNxYGQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 영화 리뷰 자료 준비하기 (온라인 자료 불러옴)\n",
        "\n",
        "+ [titles](https://raw.githubusercontent.com/brandomr/document_cluster/master/title_list.txt)\n",
        "+ [genres](https://raw.githubusercontent.com/brandomr/document_cluster/master/genres_list.txt)\n",
        "+ [synopses(wiki)](https://raw.githubusercontent.com/brandomr/document_cluster/master/synopses_list_wiki.txt)\n",
        "+ [synopses(imdb)](https://raw.githubusercontent.com/brandomr/document_cluster/master/synopses_list_imdb.txt)"
      ],
      "metadata": {
        "id": "viUftP9bYoKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 1단계 처리: 사이트에서 상위 100개의 자료를 각각 가져오기\n",
        "url = \"https://raw.githubusercontent.com/brandomr/document_cluster/master/title_list.txt\"\n",
        "titles = requests.get(url).text.split('\\n')\n",
        "titles = titles[:100]\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/brandomr/document_cluster/master/genres_list.txt\"\n",
        "genres = requests.get(url).text.split('\\n')\n",
        "genres = genres[:100]\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/brandomr/document_cluster/master/synopses_list_wiki.txt\"\n",
        "synopses_wiki = requests.get(url).text.split('\\n BREAKS HERE')\n",
        "synopses_wiki = synopses_wiki[:100]\n",
        "# cleaning\n",
        "synopses_clean_wiki = []\n",
        "for text in synopses_wiki:\n",
        "    text = BeautifulSoup(text, 'html.parser').getText()\n",
        "    #strips html formatting and converts to unicode\n",
        "    synopses_clean_wiki.append(text)\n",
        "synopses_wiki = synopses_clean_wiki\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/brandomr/document_cluster/master/synopses_list_imdb.txt\"\n",
        "synopses_imdb = requests.get(url).text.split('\\n BREAKS HERE')\n",
        "synopses_imdb = synopses_imdb[:100]\n",
        "# cleaning\n",
        "synopses_clean_imdb = []\n",
        "for text in synopses_imdb:\n",
        "    text = BeautifulSoup(text, 'html.parser').getText()\n",
        "    #strips html formatting and converts to unicode\n",
        "    synopses_clean_imdb.append(text)\n",
        "synopses_imdb = synopses_clean_imdb\n",
        "\n",
        "#@markdown 2단계처리: wiki와 imdb의 synopsis 합치기\n",
        "synopses = []\n",
        "for i in range(len(synopses_wiki)):\n",
        "    item = synopses_wiki[i] + synopses_imdb[i]\n",
        "    synopses.append(item)\n",
        "synopses[0]\n",
        "\n",
        "#@markdown 3단계 처리: 영화순위 저장\n",
        "# generates index for each item in the corpora (in this case it's just rank) and I'll use this for scoring later\n",
        "ranks = []\n",
        "for i in range(0,len(titles)):\n",
        "    ranks.append(i)\n",
        "\n",
        "\n",
        "#@markdown 4단계 처리: 영화제목, 줄거리, 장르, 순위가 몇 개씩 들어왔는지 확인\n",
        "print(\"=\"*50)\n",
        "print(\"자료요약\")\n",
        "print(\"=\"*50)\n",
        "print(str(len(titles)) + ' titles')\n",
        "print(str(len(synopses)) + ' synopses')\n",
        "print(str(len(genres)) + ' genres')\n",
        "print(str(len(ranks)) + ' ranks')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sydRxvLIZBrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##data cleaning"
      ],
      "metadata": {
        "id": "KRGSxB0aa4u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 1단계: NLTK에서 stopwords와 stemmer 가져오기\n",
        "# load nltk's English stopwords as variable called 'stopwords'\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "#@markdown 2단계: tokenize with stemming위한 함수 만들기\n",
        "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
        "def tokenize_and_stem(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "    return stems\n",
        "\n",
        "def tokenize_only(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    return filtered_tokens"
      ],
      "metadata": {
        "cellView": "form",
        "id": "J0OmNHgEa9RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Stemming한 함수적용하여 단어리스트 생성 및 확인\n",
        "nltk.download('punkt')\n",
        "totalvocab_stemmed = []\n",
        "totalvocab_tokenized = []\n",
        "for i in synopses:\n",
        "    allwords_stemmed = tokenize_and_stem(i)\n",
        "    totalvocab_stemmed.extend(allwords_stemmed)\n",
        "    \n",
        "    allwords_tokenized = tokenize_only(i)\n",
        "    totalvocab_tokenized.extend(allwords_tokenized)\n",
        "\n",
        "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
        "vocab_frame"
      ],
      "metadata": {
        "id": "CqtBqdHXb3IN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-means clustering"
      ],
      "metadata": {
        "id": "oogVU0uuf9Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ▶️ TF-IDF이용 벡터로 변환; 빈도가 너무 많거나 적은 것은 배제\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
        "                                 min_df=0.2, stop_words='english',\n",
        "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(synopses)\n",
        "\n",
        "#@markdown ▶️ TF-IDF에 이용된 단어리스트 확인 (312302단어 > 563단어)\n",
        "terms = tfidf_vectorizer.get_feature_names_out()\n",
        "print(terms)\n",
        "print(tfidf_matrix.shape)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0NPtUctyf0L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ▶️ 코사인 유사도를 이용한 거리 계산 (100개의 무비리뷰가 563차원에서 각각 한 점으로 표현가능; 점들 간의 거리계산)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "dist = 1 - cosine_similarity(tfidf_matrix)\n",
        "\n",
        "print(\"자료 결과 크기:\", dist.shape)\n",
        "print(\"=\"*50)\n",
        "dist"
      ],
      "metadata": {
        "cellView": "form",
        "id": "peiBTY-hqKZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 군집을 5개로 정하여 군집분석\n",
        "from sklearn.cluster import KMeans\n",
        "num_clusters = 5\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "km.fit(tfidf_matrix)\n",
        "clusters = km.labels_.tolist()\n",
        "# clusters\n",
        "\n",
        "import pandas as pd\n",
        "films = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
        "frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster', 'genre'])\n",
        "\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"1. 군집별별로 묶인 영화수\")\n",
        "print(\"=\"*50)\n",
        "print(frame['cluster'].value_counts())\n",
        "print(\"=\"*50)\n",
        "print(\"2. 군집별 순위(Rank) 평균\")\n",
        "print(\"=\"*50)\n",
        "grouped = frame['rank'].groupby(frame['cluster'])\n",
        "print(grouped.mean())\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"3. 분류 결과 보기\")\n",
        "print(\"=\"*50)\n",
        "frame\n"
      ],
      "metadata": {
        "id": "98QtRNy7jfxX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 각 군집별 주요단어와 영화목록 확인\n",
        "print(\"Top terms per cluster:\")\n",
        "print()\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "for i in range(num_clusters):\n",
        "    print(\"Cluster %d words:\" % i, end='')\n",
        "    for ind in order_centroids[i, :6]:\n",
        "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
        "    print()\n",
        "    print()\n",
        "    print(\"Cluster %d titles:\" % i, end='')\n",
        "    for title in frame.loc[i]['title'].values.tolist():\n",
        "        print(' %s,' % title, end='')\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lTXZUaGrkGKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 군집별 키워드를 이용하여, 군집별 이미지를 AI로 그림 (Midjourney 이용)\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [\"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.15.png\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.16.png\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.17.png\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.18.png\",\n",
        "        \"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.19.png\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 6)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"800\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ],
      "metadata": {
        "id": "JkaXTkehtRr_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multidimensional scaling (시각화를 위한 차원 축소 563 -> 2)"
      ],
      "metadata": {
        "id": "lH_TQB26k-aO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 관련 package불러온 후 MDS실행\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from sklearn.manifold import MDS\n",
        "\n",
        "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
        "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
        "xs, ys = pos[:, 0], pos[:, 1]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rsVjrXcVk_SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 군집의 색깔과 이름 지정\n",
        "#set up colors per clusters using a dict\n",
        "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
        "\n",
        "#set up cluster names using a dict\n",
        "cluster_names = {0: 'Family, home, war', \n",
        "                 1: 'Police, killed, murders', \n",
        "                 2: 'Father, New York, brothers', \n",
        "                 3: 'Dance, singing, love', \n",
        "                 4: 'Killed, soldiers, captain'}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "72PfJbFFlbY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown MDS시각화\n",
        "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
        "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
        "\n",
        "#group by cluster\n",
        "groups = df.groupby('label')\n",
        "\n",
        "# set up plot\n",
        "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
        "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
        "\n",
        "#iterate through groups to layer the plot\n",
        "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
        "for name, group in groups:\n",
        "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
        "    ax.set_aspect('auto')\n",
        "    ax.tick_params(\\\n",
        "        axis= 'x',          # changes apply to the x-axis\n",
        "        which='both',      # both major and minor ticks are affected\n",
        "        bottom='off',      # ticks along the bottom edge are off\n",
        "        top='off',         # ticks along the top edge are off\n",
        "        labelbottom='off')\n",
        "    ax.tick_params(\\\n",
        "        axis= 'y',         # changes apply to the y-axis\n",
        "        which='both',      # both major and minor ticks are affected\n",
        "        left='off',      # ticks along the bottom edge are off\n",
        "        top='off',         # ticks along the top edge are off\n",
        "        labelleft='off')\n",
        "    \n",
        "ax.legend(numpoints=1)  #show legend with only 1 point\n",
        "\n",
        "#add label in x,y position with the label as the film title\n",
        "for i in range(len(df)):\n",
        "    ax.text(df.loc[i]['x'], df.loc[i]['y'], df.loc[i]['title'], size=8)\n",
        "    \n",
        "plt.show() #show the plot"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NFHB9RrMlsdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hierarchical document clustering(dendrogram)"
      ],
      "metadata": {
        "id": "7TZ7VFAEl7Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Dendrogram을 이용한 시각화\n",
        "from scipy.cluster.hierarchy import ward, dendrogram\n",
        "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
        "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
        "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=titles);\n",
        "plt.tick_params(\\\n",
        "    axis= 'x',          # changes apply to the x-axis\n",
        "    which='both',      # both major and minor ticks are affected\n",
        "    bottom='off',      # ticks along the bottom edge are off\n",
        "    top='off',         # ticks along the top edge are off\n",
        "    labelbottom='off')\n",
        "plt.tight_layout() #show plot with tight layout"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E_1nJZfwl9Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 감사합니다 😊\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "\n",
        "def on_button_click(button):\n",
        "    sn = int(button.description) - 1\n",
        "    image.value = requests.get(urls[sn]).content\n",
        "\n",
        "urls = [\"https://raw.githubusercontent.com/junkyuhufs/workshop/main/slide.23.PNG\"\n",
        "]\n",
        "\n",
        "button_layout = widgets.Layout(width='50px', height='30px')\n",
        "\n",
        "buttons = [widgets.Button(description=str(i), layout=button_layout) for i in range(1, 2)]\n",
        "for button in buttons:\n",
        "    button.on_click(on_button_click)\n",
        "\n",
        "image = widgets.Image(value=requests.get(urls[0]).content, width=\"800\", height=\"600\")\n",
        "\n",
        "display(widgets.HBox([image, widgets.VBox(buttons)]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Gl2eOJAArw0J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}